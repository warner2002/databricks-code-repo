{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d7f53f-32c8-40ba-982d-04c7548932b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Simply say- We are going to learn...\n",
    "next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a76be2-1041-4fc6-b94b-ab46b78e169c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9435574f-1e19-4ddd-acc4-b5f11c5fcd5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Passive Data Munging** - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "\n",
    "**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b87b875-2e4d-41ad-883c-1c111a8eecab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Stage1](stage1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc009f1a-5864-4b7b-8697-1dd72790eb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging - \n",
    "- Visible - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76842c2-34fc-43a7-8138-e331de2af1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality \n",
    "- - Null columns & null rows are there\n",
    "- - duplicate rows & Duplicate keys\n",
    "- - format issues are there (age is not in number format eg. 7-7)\n",
    "- - Uniformity issues (Artist, artist)\n",
    "- - Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- - Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ccadc2-69ed-4744-8673-b83a29cd0eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1623cdd4-18ac-432b-a988-f99ab46e73e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We have to define Spark Session to enter into Spark application\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"we47_Bread_n_Butter2_Important_Application\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b7e978-9f46-4506-8ab2-c79f8bcf1089",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766808978576}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data Exploration programatically\n",
    "rawdf1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870e142d-523c-49a3-8c70-1cbd96d00377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive Munging - EDA of schema/structure functions we can use\n",
    "rawdf1.printSchema()#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(rawdf1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(rawdf1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9ed1b9-96c1-4f58-939a-320911e29b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive Datamunging - EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - Standard deviation tells you how much the data varies from the average (mean).\n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccb7e0e-aeec-4789-aecd-61187cacd34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a231c83-377c-455d-b0d4-665fc66bf9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Questions related to multiple files/paths/sub path handling\n",
    "#I have data in different filenames in a single/multiple location, i need to read all these data in a df - path=[\"path1/file1\",\"path1/file2\",\"path2/file3\"]\n",
    "#I have data in single pattern of file names in a single/multiple locations or subfolders, i need to read all these data in a df - path=[\"path1/\",\"path1/\",\"path2/\"], pathGlobFilter=\"custsm*\", recursiveFileLookup=True\n",
    "\n",
    "#Questions related handling evolving data structure with data ingested in different days/periods - Ans. Schema Evolution\n",
    "#Evolution is growth over the time (Filesystem level).. Eg. Source is sending data with additional columns week over week in csv format\n",
    "#1. Read and write in Serialized format( ORC,Parquet)\n",
    "#2. Read DF with mergeSchema = True\n",
    "\n",
    "#Questions related handling data from different sources with different related structure in a same day - Ans. Schema Merging/Melting (Dataframe level).. Eg. Source1 is sending custsmodified_NY with 5 columns and Source2 is sending custsmodified TX with 4 columns\n",
    "#1. Read file1 in DF1, read file2 in DF2\n",
    "#2. Create DF3 by merging DF1 and DF2 using df1.unionByName(df2,allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c30b1f-5451-4ca3-855e-cb218b514988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Combining Data + Schema Evolution/Merging (Structuring) - Preliminary Datamunging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbd0c39-1220-45a5-96dd-8ac93d4990cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\")\n",
    "#2. Multiple files (with different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",\"/Volumes/we47catalog/we47schema/we47_volume/custsmodifiedsample.txt\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/we47catalog/we47schema/we47_volume/\",\"/Volumes/we47catalog/we47schema/we47_volume/we47_dir1\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d3ddef-c256-474f-b8de-cefad4bdb5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#VERY VERY IMPORTANT FOR PROACTIVE DRIVING OF INTERVIEW\n",
    "#Active Data munging...\n",
    "#When you go for Schema Merging/Melting and Schema Evolution?\n",
    "#Schema Merging/Melting (unionByName,allowMissingColumns)- If we get multiple files\n",
    "#Schema Evolution (orc/parquet with mergeSchema) - If no. of columns are keeps added by the source system\n",
    "#when we know structure of the file already - schema merge/ schema not known earlier  - schema evolution\n",
    "\n",
    "#COMBINING OR SCHEMA MERGING or SCHEMA MELTING of Data from different sources(Important interview question also as like schema evolution...)\n",
    "#4. Multiple files with different structure in multiple paths or sub paths\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "display(rawdf1)\n",
    "display(rawdf2)\n",
    "rawdf_merged=rawdf1.union(rawdf2)#Use union only if the dataframes are having same columns in the same order with same datatype..\n",
    "display(rawdf_merged)\n",
    "#Expected right approach to follow\n",
    "rawdf_merged=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged)\n",
    "\n",
    "#Lets try to achieve using schema evolution(but not applicable for this data munging activity of different dataframes available at the same time, we just get the dataframes merged/unioned without writing into serialized format)\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "rawdf1.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/targetorc1/\",mode='overwrite')#Useful for future over the time\n",
    "rawdf2.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/targetorc1/\",mode='append')\n",
    "rawdf_evolved=spark.read.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/targetorc1/\",mergeSchema=True)\n",
    "display(rawdf_evolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee2be30-7760-4f90-b0e3-90be3b38a04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Just for the simple learning of schema evolution & schema merging/melting<br>\n",
    "Schema merging/melting<br>\n",
    "1,rajeshwari day1(source1)<br>\n",
    "1,rajeshwari,30 day1(source2)<br>\n",
    "\n",
    "Schema evolution<br>\n",
    "1,rajeshwari day1<br>\n",
    "1,rajeshwari,30 day2<br>\n",
    "\n",
    "Output is same in both cases...<br>\n",
    "id,name,age<br>\n",
    "1,rajeshwari,null<br>\n",
    "1,null,30<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e04175e7-a081-4ed6-873c-95e6d32d67e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4c6278-9af0-413b-8265-f7fb2ac26590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Validation - Exploration of data by actively applying cleansing & scrubbing\n",
    "Identifying and filling gaps & Cleaning data to remove outliers and inaccuracies\n",
    "Preprocessing, Preparation\n",
    "Cleansing (removal of unwanted datasets eg. na.drop),\n",
    "Scrubbing (convert of raw to tidy na.fill or na.replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5760eab-394a-4d16-afd4-a6d39e371cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cb63a5-a86b-47ff-b7ad-fdbe47980a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#Validation by doing cleansing\n",
    "strt1=\"id int, firstname string, lastname string, age int, profession string\"#applying schema strictly to validate and cleanse the data\n",
    "strt11=StructType([StructField(\"id\",IntegerType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"profession\",StringType(),True)])\n",
    "dfmethod1=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False)\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato - scrubbing)\n",
    "display(dfmethod1)\n",
    "print(\"entire count of data\",dfmethod1.count())\n",
    "print(\"after scrubbing, count of data\",len(dfmethod1.collect()))\n",
    "#or\n",
    "\n",
    "#method2 - drop malformed rows\n",
    "strt1=\"id int, firstname string, lastname string, age int, profession string\"#applying schema strictly to validate and \n",
    "dfmethod2=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"dropMalformed\",header=False)\n",
    "#We are removing the entire row, where ever data format mismatch is there (number of columns, data type mismatch) (throwing away the entire potato or some portion of it by - cleansing)\n",
    "display(dfmethod2)\n",
    "print(\"entire count of data\",dfmethod2.count())\n",
    "print(\"after cleansing, count of data\",len(dfmethod2.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "620fdbbd-f94f-4ff2-8f26-19ba4f8465d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#If we follow method1 (permissive with strict schema verification) or method2 (drop malformed with strict schema verification)\n",
    "#Challenges we have in this method1 and 2 is - unknown data loss at column level or at the row level\n",
    "#method3 best methodology of applying active data munging\n",
    "dfmethod3=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False,inferSchema=True)\n",
    "#or\n",
    "print(dfmethod3.schema)\n",
    "strt11=StructType([StructField(\"id\",StringType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",StringType(),True),StructField(\"profession\",StringType(),True)])\n",
    "dfmethod3=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False)\n",
    "#Validation by doing cleansing (not at the time of creating Dataframe, rather we will clean and scrub subsequently)...\n",
    "display(dfmethod3)#We are going to validate the raw data without any scrubbing or cleansing applied, we will decide the cleansing/scrubbing strategy in the below stages...\n",
    "print(\"entire count of data\",dfmethod3.count())\n",
    "print(\"after cleansing, count of data\",len(dfmethod3.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e884c46e-7628-4436-98de-7460c642c9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Rejection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfccf0d-da18-48aa-8329-c3f15924c51a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766816555750}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before actively Cleansing or Scrubbing - We have to create a Rejection Strategy to reduce data challenges in the future\n",
    "strt11=StructType([StructField(\"id\",IntegerType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"profession\",StringType(),True),StructField(\"corruptdata\",StringType())])\n",
    "dfmethod3=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False,columnNameOfCorruptRecord=\"corruptdata\")\n",
    "display(dfmethod3)\n",
    "print(\"entire count of data\",dfmethod3.count())\n",
    "df_reject=dfmethod3.where(\"corruptdata is not null\")\n",
    "df_reject.drop(\"corruptdata\").write.mode(\"overwrite\").csv(\"/Volumes/workspace/wd36schema/ingestion_volume/rejects/\",mode=\"overwrite\")\n",
    "print(\"Data to reject or update the source\",len(dfmethod3.where(\"corruptdata is not null\").collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf841cb-a560-4535-8fe1-480a11915ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing\n",
    "It is a process of cleaning/removing or making the data more clean Eg. Cutting/removing debris portion of the potato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "215273b4-03ac-4e9d-8d13-31e4503bafd4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "3": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766818507325}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 3
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2, but we can do cleansing and scrubbing in a controlled fashion by applying functions on the method3 dataframe\n",
    "#Important na functions we can use to do cleansing\n",
    "strt11=StructType([StructField(\"id\",StringType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",StringType(),True),StructField(\"profession\",StringType(),True)])\n",
    "dfmethod3=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False)\n",
    "display(dfmethod3.take(15))\n",
    "print(\"Actual DF count\",len(dfmethod3.collect()))\n",
    "cleansed_df1=dfmethod3.na.drop(how=\"any\")#drop the row, if any one column in our df row contains null\n",
    "cleansed_df1=dfmethod3.na.drop(how=\"any\",subset=[\"id\",\"age\"])#drop the row, if any one column id/age contains null\n",
    "print(\"cleansed any DF count\",len(cleansed_df1.collect()))\n",
    "display(cleansed_df1.take(15))\n",
    "cleansed_df2=dfmethod3.na.drop(how=\"all\")#drop the row, if all the columns in our df row contains null\n",
    "cleansed_df2=dfmethod3.na.drop(how=\"all\",subset=[\"id\",\"profession\"])#drop the row, if all the columns (id,profession) in our df row contains null\n",
    "print(\"cleansed all DF count\",len(cleansed_df2.collect()))\n",
    "display(cleansed_df2.take(15))\n",
    "\n",
    "#Threshold - least bother (if we need minimum this many number of columns with not nulls, then only we will keep the row)\n",
    "cleansed_df3=dfmethod3.na.drop(thresh=4,subset=[\"id\",\"firstname\",\"age\",\"profession\"])\n",
    "print(\"cleansed threshold DF count\",len(cleansed_df3.collect()))\n",
    "display(cleansed_df3.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "501db613-f37e-409b-b733-5803444e2bc4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767380319109}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before scrubbing, lets take the right cleansed data with id as null and entire row as null removed out\n",
    "#Finally I am arriving for our current data, lets perform the best cleansing\n",
    "cleansed_df=cleansed_df3.na.drop(subset=[\"id\"]).na.drop(how=\"all\")\n",
    "cleansed_df=cleansed_df.na.drop(subset=[\"firstname\",\"lastname\"],how='all')\n",
    "print(\"Final cleansed DF\",len(cleansed_df.collect()))\n",
    "display(cleansed_df.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44cc3ee-eaab-4159-b2ac-2327336e69ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Scrubbing\n",
    "It is a process of polishing/reformatting data or making the data more tidy Eg. polishing/scrubbing the mud portion of the potato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8937f26-3c3e-401a-bdf5-367d5bada163",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766819109645}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scrubbing will not remove the rows rather it will try to polish/scrub/reformat of the data in a usable format\n",
    "#Can be achieved using 2 functions under na (na.fill & na.replace)\n",
    "scrubbed_df1=cleansed_df.na.fill(\"na\",subset=[\"firstname\",\"lastname\"]).na.fill(\"not provided\",subset=[\"profession\"])\n",
    "scrubbed_df2=scrubbed_df1.na.replace(\"IT\",\"Information Technologies\",subset=[\"profession\"]).na.replace(\"Pilot\",\"Aircraft Pilot\",subset=[\"profession\"])\n",
    "#or\n",
    "#Find and Replace functionality\n",
    "dict1={\"IT\":\"Information Technologies\",\"Pilot\":\"Aircraft Pilot\",\"Actor\":\"Celebrity\"}\n",
    "scrubbed_df=scrubbed_df1.na.replace(dict1,subset=[\"profession\"])\n",
    "#scrubbed_df2=scrubbed_df1.na.replace(\"IT\",\"Information Technologies\",subset=[\"profession\"]).na.replace(\"Pilot\",\"Aircraft Pilot\",subset=[\"profession\"])\n",
    "print(\"scrubbed DF\",len(scrubbed_df.collect()))\n",
    "display(scrubbed_df.take(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd558ea-4a3e-427b-999c-891450d6de77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e156351e-474c-4880-9717-bf4ae17a22f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Standardization - \n",
    "Making the data more standard by adding/removing/reordering columns as per the expected standard, unifying into expected format, converting the type as expected etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a6febb4-3ca8-4a9f-8f17-651cff1f6aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization1 - Column Enrichment (Addition of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d82f7a-b464-4375-ad06-7d2826d8a1cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap\n",
    "#Standardization1 - Add columns (ENRICHMENT) to make this dataframe more standard for understanding of the source system\n",
    "standard_df1=scrubbed_df.withColumn(\"sourcesystem\",lit(\"Retail\"))#If we have to add a columns with some hardcoded value in dataframe, we have use lit function to add a hardcoded/literal value\n",
    "display(standard_df1.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7293f4-870f-4dd4-a7d1-4d707fac5d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization2 - Column Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f93ada-7028-4d03-bbc3-aa7e70b0ccac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Standardization2 - UNIFORMITY of the data\n",
    "display(standard_df1.groupBy(\"profession\").count())#DSL\n",
    "#standard_df1.createOrReplaceTempView(\"view1\")\n",
    "#display(spark.sql(\"select profession,count(1) from view1 group by profession\"))#Declarative lang\n",
    "standard_df2=standard_df1.withColumn(\"profession\",initcap(\"profession\"))#If we have to add a columns with some hardcoded value in dataframe, we have use lit function to add a hardcoded/literal value\n",
    "display(standard_df2.take(15))\n",
    "display(standard_df2.groupBy(\"profession\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904e1682-372a-4932-b5d3-a8ffdd3ff218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization3 - Format Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542b4370-af9d-444b-8749-ec929d03e4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Standardization3 - Format Standardization\n",
    "from pyspark.sql.functions import *\n",
    "cid_standardization={\"one\":\"1\",\"two\":\"2\",\"ten\":\"10\"}#We can think of using GenAI here later\n",
    "standard_df3=standard_df2.na.replace(cid_standardization,subset=[\"id\"])#Using munging feature for standardizing the data\n",
    "standard_df3=standard_df3.withColumn(\"age\",regexp_replace(\"age\",\"-\",\"\"))\n",
    "display(standard_df3.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3780a193-ea8d-4585-af19-3cad6bdccfeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization4 - Type Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc758063-bbc8-4b0a-93ab-39f4311bde06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#I wanted to learn/test a function functionality\n",
    "#create a dummy dataframe, apply function to that dummy df to test your function's functionality\n",
    "spark.sql(\"select '123' as col1\").where(\"col1 not rlike '[a-zA-Z]'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a36f12-ca3a-4c16-9fc3-7237bf2dac2f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767379587003}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df3.printSchema()\n",
    "#display(len(standard_df3.collect()))\n",
    "#display(standard_df3.where(\"id like '%trailer%'\"))\n",
    "standard_df3=standard_df3.where(\"id not rlike '[a-zA-Z]'\")#Removed the string data in the id column\n",
    "display(standard_df3.where(\"id='trailer_data:end of file'\"))\n",
    "#display(len(standard_df3.collect()))\n",
    "standard_df4=standard_df3.withColumn(\"age\",col(\"age\").cast(\"int\")).withColumn(\"id\",col(\"id\").cast(\"long\"))\n",
    "standard_df4.printSchema()\n",
    "display(standard_df4.take(15))\n",
    "#standard_df4.where(\"id=1000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461ec63e-c8a4-422d-8c1e-fb0baff8f802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization5 - Naming Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad58903c-ca26-4883-a9aa-02e38a480c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df5=standard_df4.withColumnRenamed(\"id\",\"custid\")\n",
    "standard_df5.printSchema()\n",
    "display(standard_df5.take(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90558fdc-ea11-482a-af85-a16ba8b4f59a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization6 - Reorder Standadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b541dca-eed0-4cc3-a1f3-c4be184aea43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df6=standard_df5.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcesystem\")\n",
    "standard_df6.printSchema()\n",
    "display(standard_df6.take(15))\n",
    "#standard_df6.write.mode(\"overwrite\").saveAsTable(\"munged_cust_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbcdc7d1-4a34-48c1-8958-906288eb4439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Capture all the functions or data munging functions onwards used in this entire notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1127de81-307f-46de-b55f-a440a7e254b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####DeDuplication - De-Duplication and removal of non prioritized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3515551-c415-4315-a8db-93319daf5bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Duplicate Elimination at the record level, column level and in a priority basis of some column level higher age\n",
    "dedup_df1=standard_df6.where(\"custid in (4000001,4000003)\")\n",
    "dedup_df1=standard_df6.distinct()#Eliminating Record level duplicates\n",
    "dedup_df2=dedup_df1.dropDuplicates(subset=[\"custid\"])#Retains only the first row and eliminate the subsequent rows with duplicate keys without having any other priority at the age or any other columns\n",
    "#dedup_df1.where(\"custid in (4000001,4000003)\").orderBy([\"custid\",\"age\"],ascending=[True,False]).show()\n",
    "#dedup_df2=dedup_df1.orderBy(subset=[\"custid\",\"age\"],ascending=[True,False]).coalesce(1)#Coalesing/combining all partition data into a single partition to get the dropduplicate output as expected\n",
    "display(dedup_df2)\n",
    "dedup_df3=dedup_df2.dropDuplicates([\"custid\"])#Retains only the first row and eliminate the subsequent rows with duplicate key\n",
    "display(dedup_df3)\n",
    "#dedup_df2.where(\"custid in (4000001,4000003)\").show()\n",
    "#How to cleanse the data with two same duplicate rows, except one of the column is different value?\n",
    "#we have 5 columns, only profession column is varying?\n",
    "#4000003\t41\tmohamed\tirfan\tInformation Technologies N\n",
    "#4000003 41\tmohamed\tirfan\tReporter Y\n",
    "dedup_df4=dedup_df1.dropDuplicates([\"custid\",\"age\",\"firstname\",\"lastname\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c280deb-94bd-4cec-9f11-f234c321ae36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**2. Data Enrichment** - Detailing of data\n",
    "Makes your data rich and detailed <br>\n",
    "a. Add (withColumn,select,selectExpr), Derive (withColumn,select,selectExpr), Remove/Eliminate (drop,select,selectExpr), Rename (withColumnRenamed,select,selectExpr), Modify/replace (withColumn, select/selectExpr) - (very important spark sql functions) <br>\n",
    "b. split, merge/Concat <br>\n",
    "c. Type Casting, reformat & Schema Migration <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7b9f4e-feb9-4822-b8bc-e32034aa1de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Stage2](stage2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ae37bb-f160-4f09-a953-813aee225be1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766900197501}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before we enrich, lets do some EDA\n",
    "#Every stages we need to do basic EDA (Data Exploration)\n",
    "dedup_df3.printSchema()\n",
    "print(\"Records got cleaned/munged \",dfmethod1.count()-len(dedup_df3.collect()))\n",
    "display(dedup_df3.summary())\n",
    "display(dedup_df3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9589bfe8-46f2-4842-85f6-2ff5b7cbaf89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####a. Add (withColumn,select,selectExpr), Derive (withColumn,select,selectExpr), Rename (withColumnRenamed,select,selectExpr), Modify/replace (withColumn, select/selectExpr), Remove/Eliminate (drop,select,selectExpr) - (very important spark sql functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bed3c28-1fe3-4131-86cd-51f7faa0ef94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Adding of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260a5589-bba2-4664-af8e-6672c6b1ccb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Adding columns\n",
    "datadt=\"25-12-25\"#source system data generated date\n",
    "enrich_df1=dedup_df3.withColumn(\"loaddt\",current_date()).withColumn(\"datadt\",lit(datadt))\n",
    "enrich_df1.show(2)\n",
    "enrich_df1=dedup_df3.withColumns({\"loaddt\":current_date(),\"datadt\":lit(datadt)})\n",
    "enrich_df1.show(2)\n",
    "#or\n",
    "enrich_df1=dedup_df3.withColumn(\"sourcesystem1\",col(\"sourcesystem\"))#Adding an existing column in a new column (copying a column value to a new column)\n",
    "enrich_df1.show(2)\n",
    "#or\n",
    "enrich_df1=dedup_df3.select(\"*\",current_date().alias(\"loaddt\"),lit(datadt).alias(\"datadt\"))#pure DSL\n",
    "enrich_df1.show(2)\n",
    "#or\n",
    "#colvar=f\"{datadt} as datadt\"\n",
    "datadt=\"25-12-25\"\n",
    "enrich_df1=dedup_df3.selectExpr(\"*\",\"current_date() as loaddt\",f\"'{datadt}' as datadt\")#Only select Function that supports both DSL and SQL\n",
    "enrich_df1.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e984c43e-3f76-45f0-bf1d-eeca1c67e588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Deriving of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7767295d-bf56-4590-8e6e-6e630ca39871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrich_df2=enrich_df1.withColumn(\"profession_flag\",substring(\"profession\",1,1))\n",
    "enrich_df2.show(10)\n",
    "#or we can achieve using select\n",
    "enrich_df2=enrich_df1.select(\"*\",substring(\"profession\",1,1).alias(\"profession_flag\"))\n",
    "enrich_df2.show(10)\n",
    "#or we can achieve using selectExpr\n",
    "enrich_df2=enrich_df1.selectExpr(\"*\",\"substr(profession,1,1) as profession_flag\")\n",
    "enrich_df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286ef171-2836-41c5-9d8d-7ce79d0cd278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Renaming of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03076167-e4bb-4088-ad3c-1352061cc529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrich_df3=enrich_df2.withColumnRenamed(\"profession_flag\",\"proflag\")#better to use\n",
    "enrich_df3.show(10)\n",
    "enrich_df3=enrich_df2.withColumnsRenamed({\"profession_flag\":\"proflag\",\"profession\":\"prof\"})\n",
    "enrich_df3.show(10)\n",
    "#or\n",
    "enrich_df3=enrich_df2.select(\"*\",col(\"profession\").alias(\"prof\"))#This will derive a new column called prof\n",
    "enrich_df3=enrich_df2.select(\"custid\",\"age\",\"firstname\",\"lastname\",col(\"profession\").alias(\"prof\"),\"sourcesystem\",\"loaddt\",\"datadt\",col(\"profession_flag\").alias(\"proflag\"))#This will derive a new column called prof#better to use\n",
    "#enrich_df3=enrich_df3.drop(\"profession\")#column orders are changed, not good to use\n",
    "enrich_df3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1b3ae9-5726-4550-8c24-f9bb9ecbb979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Modify/replace (withColumn, select/selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00edec7-2d17-46f8-8a18-c19291fd916f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrich_df3.printSchema()#datadt is not in a expected date format for further usage and i wanted to convert sourcesystem into uppercase\n",
    "enrich_df4=enrich_df3.withColumn(\"sourcesystem\",upper(col(\"sourcesystem\"))).withColumn(\"datadt\",to_date(col(\"datadt\"),'dd-MM-yy'))#This time withColumn didnt added a new column, it just modified the existing column\n",
    "enrich_df4.printSchema()#datadt is expected date format\n",
    "enrich_df4.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6b4370-a9f6-4b07-b703-5ddc639f74e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Remove/Eliminate (drop,select,selectExpr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce618f2-34a8-464e-ac2f-944522c576dc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766906802087}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrich_df5=enrich_df4.withColumn(\"fullname\",concat(col(\"firstname\"),lit(\" \"),col(\"lastname\")))\n",
    "enrich_df5=enrich_df5.drop(\"firstname\",\"lastname\").select(\"custid\",\"age\",\"fullname\",\"prof\",\"sourcesystem\",\"loaddt\",\"datadt\",\"proflag\")#drop function is good to use if we just want to drop a column in the df\n",
    "display(enrich_df5.limit(10))\n",
    "#or\n",
    "enrich_df5=enrich_df4.select(\"custid\",\"age\",concat(col(\"firstname\"),lit(\" \"),col(\"lastname\")).alias(\"fullname\"),\"prof\",\"sourcesystem\",\"loaddt\",\"datadt\",\"proflag\")#Better methodology\n",
    "display(enrich_df5.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b283d6bb-68bd-4c27-9fe1-b14d9aa2fcf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Conclusion/Best practices of using different column enrichment functions\n",
    "1. select is good to use if we want to perform - \n",
    "Good for ordering/reordering, only renaming column (not good), only reformatting/deriving a column (not good), **for all of these operation in a single iteration** such renaming, reordering, reformatting,deriving, dropping etc., (best to use)\n",
    "2. selectExpr is good to use if we want to perform - Same as select by using iso sql functionality (if we are not familiar in DSL) **for all of these operation in a single iteration**\n",
    "3. withColumn is good to use if we want to perform - \n",
    "**for adding/deriving/modifying/replacing in a single iteration**\n",
    "Adding/Deriving column(s) in the last (Good), Modifying/replacing (Good), Renaming (not good), Dropping(not possible)\n",
    "4. withColumnRenamed is good to use if we want to perform - only for renaming column (Good)\n",
    "5. drop is good to use if we want to perform - only dropping of columns in the given position (Good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "220285d0-8353-4a44-9c3a-e07d82c8537d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####b. Splitting & Merging/Melting of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ad126e-f567-4fe0-9c55-65496726774f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Splitting of columns\n",
    "enrich_df6=enrich_df5.withColumn(\"profsplit\",split(col(\"prof\"),' '))\n",
    "enrich_df6=enrich_df6.withColumns({\"proffirst\":col(\"profsplit\")[0],\"proflast\":col(\"profsplit\")[size(col(\"profsplit\"))-1]})\n",
    "display(enrich_df6.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37eaeaae-ea1d-416f-8e93-bb23eca1f10c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766909271349}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Merging of columns\n",
    "enrich_df7=enrich_df6.withColumn(\"proflag\",concat(substring(col(\"proffirst\"),1,1),substring(col(\"proflast\"),1,1))).drop(\"profsplit\")\n",
    "display(enrich_df7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91b011f-565e-4e91-98e7-f66fb5b8774a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####c. Typecasting & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25dcbd54-2aed-493c-a898-04f87266b643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Consider the below casting and formatting\n",
    "enrich_df7.printSchema()#datadt is not in a expected date format for further usage and i wanted to convert sourcesystem into uppercase\n",
    "#enrich_df4=enrich_df3.withColumn(\"sourcesystem\",upper(col(\"sourcesystem\"))).withColumn(\"datadt\",to_date(col(\"datadt\"),'dd-MM-yy'))#This time withColumn didnt added a new column, it just modified the existing column\n",
    "#enrich_df4.printSchema()#datadt is expected date format\n",
    "#enrich_df4.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a13909-ec09-4e2b-ae67-3e3b2c37db5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Customization & Processing - Application of Tailored Business specific Rules <br>\n",
    "a. User Defined Functions <br>\n",
    "b. Building of Frameworks & Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67386d0f-5474-4752-b267-afe2f1613ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Stage3](stage3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148ed11a-c526-4d0a-9ba7-dd37b01e997c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######1. Example udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ec4630-106b-4ae7-b1cf-ea3f81435fe4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767414364600}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Import udf library\n",
    "from pyspark.sql.functions import udf\n",
    "#2. Create a Python function - Can only work in single local computer\n",
    "def convert_to_upper(name):#python function can only run in irfan's computer\n",
    "    return name.upper()\n",
    "#3. Register the python function as user defined function, hence it supports distributed processing of data on dataframe columns\n",
    "user_def_fun=udf(convert_to_upper)\n",
    "#4. Apply the udf in my dataframe columns - enrich_df7(10k) -> nandha 5k, senthil 5k\n",
    "enrich_df8=enrich_df7.withColumn(\"fullname\",user_def_fun(col(\"fullname\")))\n",
    "display(enrich_df8.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df863fdf-f9da-4564-83cd-019f5fbeba28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######2. Interview Question: Have you created any custom/udfs in your project, if so what you did out of that, is it good to use?\n",
    "1. Yes I created for some custom/business specific requirement, which has some complex logics to achive which is not achievable in a regular sql query. for eg. taxation business rule is very specific to my organization/state.\n",
    "or\n",
    "No, We are not suppose to create udfs as it affects the performance of the pipeline, hence we tried implementing most of business logics in our sql itself/ We created very limited udfs considering performance bottleneck only if it is inevitable.\n",
    "\n",
    "Interview Question: Why UDFs are creating performance bottleneck?\n",
    "Ans: UDFs are blackbox for the Spark optimizer engine (Catalyst/Photon) can't understand the function that we created. Refer the code below.\n",
    "\n",
    "is there any exceptional mechanism in spark to resolve the performance issue if we go with custom function ? If we use udf, there is no option to fix this performance issue, try to convert udf to sql functionality..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4103053b-57ec-41bf-90d0-657132484026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Builtin function\")\n",
    "rawdf1.withColumn(\"upperfname\",upper(\"firstname\")).explain()\n",
    "#== Photon Explanation ==\n",
    "#The query is fully supported by Photon/Catalyst.\n",
    "\n",
    "print(\"Custom UDF function\")\n",
    "rawdf1.withColumn(\"upperfname\",user_def_fun(\"firstname\")).explain()\n",
    "#Photon/Catalyst does not fully support the query because:\n",
    "#Unsupported node: BatchEvalPython [convert_to_upper(firstname#19759)#19778], [pythonUDF0#19781].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44264919-55d4-4b56-89ed-722f92aec476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######3. Agecategory User defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f954c47-1267-4ebd-8fa8-0522a0493fdc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768013711846}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lets Create an UDF to calculate agecateogy of customers\n",
    "#1. Import udf library\n",
    "from pyspark.sql.functions import udf\n",
    "#2. Create a Python function - Can only work in single local computer\n",
    "def agecat(agecol):\n",
    "    if agecol<11:\n",
    "        return \"Children\"\n",
    "    elif agecol>=11 and agecol<=19:\n",
    "        return \"Teens\"\n",
    "    elif agecol>=20 and agecol<=50:\n",
    "        return \"Middleage\"\n",
    "    else:\n",
    "        return \"Oldage\"\n",
    "    \n",
    "#3. Register the python function as user defined function, hence it supports distributed processing of data on dataframe columns\n",
    "agecatudf=udf(agecat)#This python function is now a spark ready distributed function\n",
    "#4. Apply the udf in my dataframe columns - enrich_df7(10k) -> nandha 5k, senthil 5k\n",
    "custom_df=enrich_df7.withColumn(\"agecat\",agecatudf(col(\"age\")))\n",
    "display(custom_df.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4a471d-7764-4d45-90ed-14b778f07a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Data core Curation/Processing (Pre Wrangling) - Applying different levels of business logics, transformation, filtering, grouping, aggregation, sorting and limits applying different transformation functions\n",
    "1. Select, Filter\n",
    "2. Derive flags & Columns\n",
    "3. Format\n",
    "4. Group & Aggregate\n",
    "5. Sorting\n",
    "6. Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536d2d1f-e930-4748-b627-1f7b62923a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Curation](stage4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e2f8cfc-2894-4ae5-95f0-982f102294aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Select, Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94da226d-6c8d-45df-94e2-ed939428898f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Select - column selection, reordering, column level filtering\n",
    "selectdf=custom_df.select(\"custid\",\"age\",upper(\"prof\").alias(\"profession\"))#DSL\n",
    "selectdf.show(1)\n",
    "selectdf=custom_df.selectExpr(\"custid\",\"age\",\"upper(prof) as profession\")#SQL\n",
    "selectdf.show(1)\n",
    "#Where/Filter -We did row level filtering applying some conditions (SQL/DSL fashion)\n",
    "filterdf=selectdf.where((col(\"age\")>30) | (col(\"age\")<60))#DSL Syntax\n",
    "filterdf.show(1)\n",
    "filterdf=selectdf.where(\"age>30 or age<60\")#SQL Syntax\n",
    "filterdf.show(1)\n",
    "\n",
    "#where or filter, which one to use? Both are aliases (filter is used by fbp people and where is used by sql people)\n",
    "#display(filterdf.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2a3aba-8d0d-416a-a19b-96c36ad60878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Question : Whether we can apply select -> where or where -> select\n",
    "#Answer : Both are same, spark (photon/catalyst) will do row level filter first\n",
    "selectfilterdf=rawdf1.selectExpr(\"id\",\"age\",\"upper(profession) as profession\").where(\"age>30 or age<60\")\n",
    "selectfilterdf.show(1)\n",
    "selectfilterdf.explain()\n",
    "#or \n",
    "selectfilterdf=rawdf1.where(\"age>30 or age<60\").selectExpr(\"id\",\"age\",\"upper(profession) as profession\")\n",
    "selectfilterdf.show(1)\n",
    "selectfilterdf.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfcd3419-a3db-4871-855d-e6465d7f3ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Derive flags & Columns\n",
    "We are achieving by using case statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e35936-4afe-41d5-a35d-3f9f2fddffb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Deriving Columns (Age category)\n",
    "#Is there any exceptional mechanism in spark to resolve the performance issue if we go with custom function ? If we use udf, there is no option to fix this performance issue, try to convert udf to sql functionality...\n",
    "#We will try to achieve the same functionality of the udf by using sql functions (better option)\n",
    "'''def agecat(agecol):\n",
    "    if agecol<11:#when(condition,return something)\n",
    "        return \"Children\"\n",
    "    elif agecol>=11 and agecol<=19:\n",
    "        return \"Teens\"\n",
    "    elif agecol>=20 and agecol<=50:\n",
    "        return \"Middleage\"\n",
    "    else:\n",
    "        return \"Oldage\"\n",
    "'''\n",
    "enrich_df7.createOrReplaceTempView(\"enrichview\")\n",
    "#SQL Syntax: case when condt then expression when cond2 then expression2 else expression end\n",
    "#spark.sql(\"select *,case when age<11 then 'Children' when age>=11 and age<20 then 'Teens' when age>=20 and age<=50 then 'Middleage' else 'Oldage' end as agecat from enrichview\").show(2)\n",
    "spark.sql(\"\"\"select *,\n",
    "          case when proffirst=proflast then upper(substr(proffirst,1,2)) \n",
    "          else upper(concat(substring(proffirst,1,1),substr(proflast,1,1)))\n",
    "          end as profind \n",
    "          from enrichview\"\"\").show(2)\n",
    "\n",
    "#DSL Syntax: when(condition,return something).when((cond1)&(cond2),return something).otherwise(return something else)\n",
    "derived_df1=enrich_df7.withColumn(\"agecat\",\n",
    "                                 when((col(\"age\")<11),lit(\"Children\")).\n",
    "                                 when(((col(\"age\")>=11) & (col(\"age\")<20)),lit(\"Teens\")).\n",
    "                                 when(((col(\"age\")>=20) & (col(\"age\")<=50)),lit(\"Middleage\")).\n",
    "                                 otherwise(lit(\"Oldage\")))\n",
    "#DSL Syntax to derive indicator for profession\n",
    "derived_df2=derived_df1.withColumn(\"profind\",when(((col(\"proffirst\"))==(col(\"proflast\"))),upper(substring(col(\"proffirst\"),1,2))).otherwise(upper(concat(substring(\"proffirst\",1,1),substring(col(\"proflast\"),1,1)))))\n",
    "derived_df2.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a8a3b1d-7455-486c-be43-02dd3e657453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####3.Format (deriving columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ec5df4-fcb1-482c-bc96-ad9e8c2c19a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "formatted_derived_df=derived_df2.select(\"*\",date_format(col(\"datadt\"),'yyyyMM').alias('datadtyrmth')\n",
    "                                         ,year(\"datadt\").alias(\"datadtyr\")\n",
    "                                         ,month(\"datadt\").alias(\"datadtmth\")\n",
    "                                         ,date_diff(\"loaddt\",\"datadt\").alias(\"delayindays\")\n",
    "                                         ,last_day(\"datadt\").alias(\"datadtlastday\")\n",
    "                                         ,date_add(\"loaddt\",1).alias(\"nextday\"))\n",
    "formatted_derived_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0be528-6c7e-4bcc-a162-99dfb33940b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####4. Group & Aggregate\n",
    "Mostly used in data analytics, but Transformation also we will do<br>\n",
    "Thumb rules:<br>\n",
    "1. group by - dimension of the data, column should be low in cardinality/high frequency, it can be descriptive/flag/indicators<br>\n",
    "2. aggregation columns - measures of the data, It can high cardinal, mostly number/date data types, measurable value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba5462d-e301-48ab-857e-b59e5ee781e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "grouped_df=formatted_derived_df.groupBy(\"profind\")#It will just group\n",
    "display(grouped_df)\n",
    "#Calculate profession and agecategory wise count of customers\n",
    "grouped_agg_df=formatted_derived_df.groupBy(\"prof\",\"agecat\").count()\n",
    "grouped_agg_df.show(2)\n",
    "#Not preferred (only for single column) Calculate profession and agecategory wise avg age of customers\n",
    "grouped_agg_df=formatted_derived_df.groupBy(\"prof\").avg(\"age\").withColumnRenamed(\"avg(age)\",\"avgage\")\n",
    "grouped_agg_df.show(2)\n",
    "#More preferred to use agg(aggregate functions) Calculate profession and agecategory wise avg age of customers\n",
    "grouped_agg_df=formatted_derived_df.groupBy(\"prof\").agg(avg(\"age\").alias(\"avgage\"),min(\"age\").alias(\"minage\"),max(\"age\").alias(\"maxage\"))\n",
    "grouped_agg_df.show(10)\n",
    "grouped_agg_df=formatted_derived_df.groupBy(\"prof\",\"agecat\").agg(avg(\"age\").alias(\"avgage\"),min(\"age\").alias(\"minage\"),max(\"age\").alias(\"maxage\"))\n",
    "grouped_agg_df.show(10)#We will use this grouped aggregated data later post wrangling operation (last stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53213f6c-ae83-46c8-83e9-3b9c0a57a775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####5. Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c764b11-3692-4f6f-a735-e760f9c667ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_agg_df.orderBy(\"prof\",\"agecat\").show(20)#by default ascending order of sorting\n",
    "grouped_agg_df.orderBy([\"prof\",\"agecat\"],ascending=[True,False]).show(20)#by default ascending,descending order of sorting\n",
    "#We will use this grouped aggregated sorted data later post wrangling operation (last stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cfa05d8-6d19-4078-9599-9d8ea935098c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####6. Limit\n",
    "Difference between limit, take and show/display:\n",
    "1. limit(transformation) is used to limit the number of rows in the dataframe (can be placed anywhere)\n",
    "2. take (action) is used to limit the number of rows in the dataframe and return the result as a python list of rows (can be placed in the last)\n",
    "3. show is used to display the dataframe in a tabular format\n",
    "4. display (databricks specific) is used to display the dataframe in a formatted/advanced tabular format where we can download/visualize etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "061cb198-d30e-4ce8-b936-6986baa45d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_agg_df.orderBy([\"prof\",\"agecat\"],ascending=[True,False]).limit(10).show(20)\n",
    "grouped_agg_df.orderBy([\"prof\",\"agecat\"],ascending=[True,False]).take(10)\n",
    "display(grouped_agg_df.orderBy([\"prof\",\"agecat\"],ascending=[True,False]).take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f34fd84-c66d-4a41-8162-a88092b765ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Data Wrangling - Transformation & Analytics\n",
    "  1. Joins<br>\n",
    "    1. Frequently used simple joins (inner, left)<br>\n",
    "    2. InFrequent simple joins (self, right, full, cartesian)<br>\n",
    "    3. Advanced joins (Semi and Anti)<br>\n",
    "    4. Join Optimazation in Distributed platform(Broadcast join, SMB join, M/R side join, Shuffle Hash join, skewed join(avoided))\n",
    "  2. Lookup\n",
    "  3. Lookup & Enrichment\n",
    "  4. Schema Modeling  (Denormalization)\n",
    "  5. Windowing\n",
    "  6. Analytical\n",
    "  7. Set operations\n",
    "  8. grouping & aggregations (advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e30feaa-82b6-45fc-ac23-e6c2b04b859c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Stage5](stage5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb30f45-8134-4c96-ad4e-252083143198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1. Joins\n",
    "Joins are Relation/connection of one or more tables to perform widened (horizontal) data analytics\n",
    "1. Frequently used simple joins (inner, left)\n",
    "2. InFrequent simple joins (self, right, full, cartesian)\n",
    "3. Advanced joins (Semi and Anti)\n",
    "or<br>\n",
    "Additional surrounding concepts of Joins: Relationships(1:1/1:many/many:many), schema modeling(star/snowflakes) Normalization/denormalization,  <br>\n",
    "Categories of Joins:<br>\n",
    "1.Inner join<br>\n",
    "Type: inner/equi/natural join (important)<br>\n",
    "\n",
    "2.Outer join<br>\n",
    "a.Left (important)<br>\n",
    "b.Right (We can achieve using left outer by swapping tables)<br>\n",
    "c.Full<br>\n",
    "\n",
    "3.Non Equi Join<br>\n",
    "Type: cross/cartesian (should be avoided)<br>\n",
    "\n",
    "4. Self join (we can achieve using inner join and we do Related analytics with the eligible data joining the same table)<br>\n",
    "eg. <br>\n",
    "custid,name,refcustid<br>\n",
    "1,irfan,1<br>\n",
    "2,bhargavi,1<br>\n",
    "3,nancy,1<br>\n",
    "4,bharathi,3<br>\n",
    "\n",
    "5. Specialized left join<br>\n",
    "a. semi<br>\n",
    "b. anti<br>\n",
    "\n",
    "6. Optimized Join (This we learn later post architecture revisit)<br>\n",
    "a. broadcast/map side join<br>\n",
    "b. Sort Merge bucket join / reduce side join<br>\n",
    "c. Shuffle hash join / reduce side join<br>\n",
    "d. skewed join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3369811-de0e-42ad-ab48-1a134549da6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custdf1=formatted_derived_df\n",
    "df1=custdf1.where(\"custid in (4000100,4000101,4000108)\")\n",
    "df2=custdf1.where(\"custid in (4000100,4000102,4000103)\")\n",
    "df1=df1.withColumn(\"custid\",when((col(\"custid\")==4000108),lit(None)).otherwise(col(\"custid\")))\n",
    "df2=custdf1.where(\"custid in (4000100,4000102,4000103,4000104)\")\n",
    "df2=df2.withColumn(\"custid\",when((col(\"custid\")==4000104),lit(None)).otherwise(col(\"custid\")))\n",
    "display(df1)\n",
    "display(df2)\n",
    "print(\"inner\")\n",
    "display(df1.join(df2,how='inner',on='custid'))#1\n",
    "print(\"semi\")\n",
    "display(df1.join(df2,how='semi',on='custid'))#1\n",
    "print(\"left\")\n",
    "display(df1.join(df2,how='left',on='custid'))#2\n",
    "print(\"anti\")\n",
    "display(df1.join(df2,how='anti',on='custid'))#2\n",
    "print(\"right\")\n",
    "display(df1.join(df2,how='right',on='custid'))#3\n",
    "print(\"full\")\n",
    "display(df1.join(df2,how='full',on='custid'))#4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7ae8d6-cf38-4182-b7c0-5253f2cb94f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#Requirement: I need to do analytics/reporting of top 3 customers who did highest amount of transactions in our business last year, so i can send them some offers.\n",
    "#Solution: First Lets try to bring transaction dataset to perform joins\n",
    "strt1=\"txnid long,txndt string,custid long,amt float,cat string,prod string,city string,state string,spendby string\"\n",
    "txnsdf=spark.read.schema(strt1).csv(\"/Volumes/workspace/default/volumewe47_datalake/txns_2025.txt\",header=False)\n",
    "#print(len(txnsdf.collect()))\n",
    "txnsdf_munged=txnsdf.distinct().dropDuplicates([\"txnid\"]).na.drop()\n",
    "#print(len(txnsdf_munged.collect()))\n",
    "txnsdf_munged.printSchema()\n",
    "txnsdf2=txnsdf_munged.withColumn(\"txndt\",to_date(\"txndt\",'MM-dd-yyyy'))\n",
    "txnsdf2.printSchema()\n",
    "display(txnsdf2.summary())\n",
    "display(txnsdf2.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1178a1a4-6e12-403a-85dd-12368536117c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768013663035}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Requirement: I need to do analytics/reporting of top 3 customers who did more and highest sum amount of transactions in our business last year, so i can send them some offers.\n",
    "#First - Let us perform join to achieve the expected requirement result (one to many join)\n",
    "#Second - Do grouping & aggregation\n",
    "#Third - Sorting\n",
    "#Fourth - limiting\n",
    "custdf1=formatted_derived_df\n",
    "#custdf1.printSchema()\n",
    "joineddf=custdf1.join(other=txnsdf2,on=\"custid\",how=\"inner\")#Preferable join is \"inner\" (whether you mention or not)\n",
    "print(joineddf.count())\n",
    "joineddf.groupBy(\"custid\",\"fullname\").count().orderBy(desc(\"count\")).show(3)#Top 3 customers visited our shop mostly\n",
    "top3=joineddf.groupBy(\"custid\",\"fullname\",\"prof\").agg(sum(\"amt\").alias(\"tot_amt\"))\\\n",
    ".withColumn(\"custtype\",lit(\"top3\"))\\\n",
    ".orderBy(\"tot_amt\",ascending=False)\\\n",
    ".limit(3) #Top 3 customers purchased for highest amount last year\n",
    "least3=joineddf.groupBy(\"custid\",\"fullname\",\"prof\").agg(sum(\"amt\").alias(\"tot_amt\")).withColumn(\"custtype\",lit(\"least3\")).orderBy(\"tot_amt\",ascending=True).limit(3)#Top 3 customers purchased for lowest amount last year\n",
    "display(top3.union(least3))\n",
    "\n",
    "#or (For performance optimization interview answer)\n",
    "#First - Do grouping & aggregation\n",
    "grouped_cust_txns=txnsdf2.groupBy(\"custid\").agg(sum(\"amt\").alias(\"tot_amt\"))\n",
    "#Second - Let us perform join to achieve the expected requirement result (one to one join)\n",
    "joineddf=custdf1.join(other=grouped_cust_txns,on=\"custid\",how=\"inner\")\n",
    "#Third - Sorting\n",
    "top3=joineddf.select(\"custid\",\"fullname\",\"prof\",\"tot_amt\").withColumn(\"custtype\",lit(\"top3\"))\\\n",
    ".orderBy(\"tot_amt\",ascending=False)\n",
    "#Fourth - limiting\n",
    "top3=top3.limit(3)\n",
    "display(top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68eef54a-ef81-470a-9499-fc9190811ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Requirement: I need to do analytics/reporting of the dormant/in active customer who didn't visited our business last year\n",
    "#custdf1 dataframe contains all customer info of the complete years of my business\n",
    "#txnsdf2 dataframe contains last 1 year transactions\n",
    "custdidnotvisitedlastyeardf=custdf1.join(txnsdf2,on=\"custid\",how=\"left\")#Returns all data from left df and only matching data from right df rest of data will be having nulls\n",
    "display(custdidnotvisitedlastyeardf.where(\"txnid is null\"))#inactive customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b64b68-9761-4da9-bc41-63b8dc34df6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Requirement: I need to do analytics/reporting of the total number of active customer who visited our business last year\n",
    "#custdf1 dataframe contains all customer info of the complete years of my business\n",
    "#txnsdf2 dataframe contains last 1 year transactions\n",
    "custdidnotvisitedlastyeardf=custdf1.join(txnsdf2,on=\"custid\",how=\"left\")#Returns all data from left df and only matching data from right df rest of data will be having nulls\n",
    "print(custdidnotvisitedlastyeardf.where(\"txnid is not null\").selectExpr(\"custid\").distinct().count())#active customers of last 1 year\n",
    "\n",
    "#how many active customers of 2025 december\n",
    "txnsdfdec2025=txnsdf2.where(\"month(txndt)=12\")\n",
    "custdidnotvisiteddecmonthdf=custdf1.join(txnsdfdec2025,on=\"custid\",how=\"left\")\n",
    "print(\"inactive customers of december month\",custdidnotvisiteddecmonthdf.where(\"txnid is null\").selectExpr(\"custid\").distinct().count())\n",
    "print(\"active customers of december month\",custdidnotvisiteddecmonthdf.where(\"txnid is not null\").selectExpr(\"custid\").distinct().count())\n",
    "\n",
    "#I want achieve right outer join without using right keyword?\n",
    "custdidnotvisitedlastyeardf=txnsdf2.join(custdf1,on=\"custid\",how=\"left\")#this is right join now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07c9a0a-809f-4216-88d6-be3d8f1f742b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767505202531}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#how many products are transacted in december month regardless of customer information\n",
    "custdidnotvisiteddecmonthdf=custdf1.join(txnsdfdec2025,on=\"custid\",how=\"right\")\n",
    "display(custdidnotvisiteddecmonthdf)\n",
    "print(\"Overall number of products \",txnsdfdec2025.select(\"prod\").distinct().count())\n",
    "print(\"Products had no purchases by the inactive customers\",custdidnotvisiteddecmonthdf.where(\"age is null\").selectExpr(\"prod\").distinct().count())#active products of december month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7159dbd4-a86b-46db-bc3a-797761eef74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#how many products are transacted in december month (show me both customer and products regardless of customer information or product information matching)\n",
    "custdidnotvisiteddecmonthdf=custdf1.join(txnsdf2,on=\"custid\",how=\"full\")\n",
    "print(custdidnotvisiteddecmonthdf.count())\n",
    "#print(\"Overall number of products \",txnsdfdec2025.select(\"prod\").distinct().count())\n",
    "print(\"Total Products available \",custdidnotvisiteddecmonthdf.selectExpr(\"prod\").distinct().count())#active products of december month\n",
    "print(\"Products had purchases by customers\",custdidnotvisiteddecmonthdf.where(\"age is not null\").selectExpr(\"prod\").distinct().count())#active products of december month\n",
    "print(\"Products not purchased by customers\",custdidnotvisiteddecmonthdf.where(\"age is null\").selectExpr(\"prod\").distinct().count())#active products of december month\n",
    "print(\"Total customer available, but no transaction made \",custdidnotvisiteddecmonthdf.where(\"txnid is null\").selectExpr(\"fullname\").distinct().count())#active products of december month\n",
    "print(\"Total customer available, transaction made \",custdidnotvisiteddecmonthdf.where(\"txnid is not null\").selectExpr(\"fullname\").distinct().count())#active products of december month\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd72ab8-0565-4eb5-920d-62bdb60178c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######1. Lookup \n",
    "semi/anti (best), left/inner (least bother)\n",
    "Lookup and enrichment is the process of looking up for some data attributes using the key to identify the presence of the values (not the actual values are returned)\n",
    "lookup? existence check that returns boolean\n",
    "Eg. whether this particular customer made a transaction or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15a5531-bc98-4254-9449-1808c504b384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#I need to produce only the df1 data present in df2 (by doing lookup)\n",
    "df1=custdf1.where(\"custid in (4000100,4000101)\")\n",
    "df1.show()\n",
    "df2=custdf1.where(\"custid in (4000100,4000102,4000103)\")\n",
    "#With the left customer info provided, check whether it is present in the right df and display both left and right matching data\n",
    "display(df1.alias(\"a\").join(df2.alias(\"b\"),how='inner',on='custid').select(col(\"a.*\")))#Returns only the matching data from both left & right df also, comparing with right, hence Inner join will lookup and enrich also\n",
    "\n",
    "#With the left customer info provided, check whether it is present in the right df and display only left matching data (right)\n",
    "display(df1.join(df2,how='left_semi',on='custid'))#Returns only the matching data from the left df, comparing with right - hence semi is a lookup join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b3a863-7a22-4e9d-a454-7e6274028956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Requirement: I need to do analytics/reporting of the total number of active customer who visited our business last year\n",
    "#custdf1 dataframe contains all customer info of the complete years of my business\n",
    "#txnsdf2 dataframe contains last 1 year transactions\n",
    "custdidnotvisitedlastyeardf=custdf1.join(txnsdf2,on=\"custid\",how=\"left_semi\")#Returns all data from left df only that matches with the right df\n",
    "print(\"Total active customers of last year \",custdidnotvisitedlastyeardf.count())\n",
    "#display(custdidnotvisitedlastyeardf.where(\"txnid is not null\").selectExpr(\"custid\").distinct().count())#inactive customers of last 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005a14b0-3023-47b3-ae5b-2b0cf56c63a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Requirement: I need to do analytics/reporting of the total number of IN active customer who didn't visited our business last year\n",
    "custdidnotvisitedlastyeardf=custdf1.join(txnsdf2,on=\"custid\",how=\"leftanti\")#Returns all data from left df only that DID NOT matches with the right df\n",
    "display(custdidnotvisitedlastyeardf)\n",
    "print(\"Total IN active customers of last year \",custdidnotvisitedlastyeardf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b968a349-7b7c-4543-b8e6-fbb67d1689db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######2. Lookup & Enrichment\n",
    "Lookup and enrichment using joins - left_join (best), inner, right(least bother)\n",
    "Lookup and enrichment is the process of looking up for for some data attributes using the key and enrich the values.\n",
    "Eg. Checking the bank account balance for a given acct id.\n",
    "lookup & enrichment? existence check that returns result/value that can be used for enriching our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4eeb762-8ed6-4ac1-89c1-f9ae51d54753",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767511570414}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_7fa27725\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_84f0437d\",\"enabled\":true,\"columnId\":\"city\",\"dataType\":\"string\",\"filterType\":\"null\",\"filterConfig\":{}}],\"local\":false,\"updatedAt\":1767511605387}],\"syncTimestamp\":1767511605387}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"city\"},{\"kind\":\"literal\",\"value\":null,\"type\":\"string\"}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lookup_enriched_df=custdf1.join(txnsdf2,how='left',on='custid')\n",
    "display(lookup_enriched_df.where(\"txnid is not null\"))\n",
    "display(lookup_enriched_df.where(\"txnid is null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c76b6a9-2fcf-44ac-8ef4-bd4d9a3d9abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lookup_enriched_df=custdf1.join(txnsdf2,how='inner',on='custid')\n",
    "#display(lookup_enriched_df.where(\"txnid is not null\"))#Only the matching data between left and right alone returned\n",
    "#display(lookup_enriched_df.where(\"txnid is null\"))#no data returned\n",
    "display(lookup_enriched_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2705903c-2250-431c-b8c6-0c85da8005e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######3.Schema Modeling (Denormalization)\n",
    "Use inner(mostly),left/right/full(we can use depends on the business)\n",
    "We can build Datawarehouse components (dimension, fact tables) appling joins on the tables to achieve different types of schemas\n",
    "1. Star Schema\n",
    "2. Snowflake Schema\n",
    "3. Starflake Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c7d10d-5fc6-4c49-955d-dccccd6731a7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767512661723}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Implementing only star schema model (fact surrounded by one or more main dimension) to produce denormalized (single dataset) fact table\n",
    "dimensiondf=custdf1\n",
    "#display(dimensiondf)\n",
    "factdf=txnsdf2\n",
    "#display(factdf)\n",
    "final_denormalized_df1=dimensiondf.join(factdf,how='inner',on='custid')#applying inner to reject/miss data from both side\n",
    "final_denormalized_df2=dimensiondf.join(factdf,how='left',on='custid')#applying left outer to ensure all customer data is present\n",
    "final_denormalized_df3=dimensiondf.join(factdf,how='right',on='custid')#applying right outer to ensure all transaction data is present\n",
    "final_denormalized_df4=dimensiondf.join(factdf,how='full',on='custid')#applying full outer to not reject/miss any data\n",
    "display(final_denormalized_df1)#Final fact table to load in our lakehouse(delta table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35178645-6ff3-4c9f-818e-7d52c6c360c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####5.Windowing\n",
    "row_number, rank, dense_rank<br>\n",
    "Application of Windowing & Analytical functions<br>\n",
    "Most frequent interview questions asked...<br>\n",
    "1. how do you create surrogate/seq number in tables?<br>\n",
    "2. Top N analysis of data?<br>\n",
    "3. how do you remove duplicates in the table data?<br>\n",
    "select * from (select *,row_number() over(partition by department_id order by salary) rownum\n",
    "from employees)temp\n",
    "where rownum=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b392e1f-0218-4543-8f34-ddf84eb3447b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768022552636}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creation of Surrogate/Sequence Numbers\n",
    "#1. how do you create surrogate/seq number in tables? \n",
    "# If the source given some key (natural key) and we wanted to maintain our own key in our environment then we create surrogate key (unique key with sequence) Eg. For a bank - Aadharnumber(naturalkey) & bankacctnumber(surrogatekey)\n",
    "from pyspark.sql.window import Window\n",
    "dfcusts=final_denormalized_df1\n",
    "dfcusts_surrogate_seqnum_df=dfcusts.select(row_number().over(Window.orderBy(\"custid\",\"amt\")).alias(\"primary_sk\"),\"*\")\n",
    "#Performance degradation: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
    "#or \n",
    "#One more efficient way to create surrogate key without sorting the data (Interview question)\n",
    "dfcusts_surrogate_seqnum_df=dfcusts.select(monotonically_increasing_id().alias(\"primary_sk\"),\"*\")\n",
    "display(dfcusts_surrogate_seqnum_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4089d7-d9a3-4b73-a076-02887537c25d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767514131406}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768023397789}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Top N Analysis (Very very important for interview..)\n",
    "from pyspark.sql.window import Window\n",
    "dfcusts=final_denormalized_df1.select(\"custid\",\"fullname\",\"prof\",\"txnid\",\"amt\",\"prof\").where(\"custid in (4000001,4000002,4000003)\")\n",
    "#display(dfcusts)\n",
    "\n",
    "print(\"1.Row number\")\n",
    "custtrans_topn_df=dfcusts.withColumn(\"custrno\",row_number().over(Window.partitionBy(\"custid\").orderBy(desc(\"amt\"))))\n",
    "display(custtrans_topn_df)\n",
    "custtrans_topn_df.where(\"custrno=1\").show()#Show me the only top 1 transaction made by the every customer\n",
    "custtrans_topn_df.where(\"custrno=2\").show()#Second Highest transaction made by each customer\n",
    "custtrans_topn_df.where(\"custrno=5\").show()#Fifth Highest transaction made by each customer (row_number will return only 1 row)\n",
    "custtrans_topn_df.where(\"custrno<=3\").show()#Show me the top 3 transactions made by the customers (with the same values of transaction)\n",
    "\n",
    "custtrans_topn_df=dfcusts.withColumn(\"custrno\",row_number().over(Window.partitionBy(\"custid\").orderBy(asc(\"amt\"))))\n",
    "display(custtrans_topn_df)\n",
    "custtrans_topn_df.where(\"custrno=1\").show()#Least transaction made by each customer\n",
    "custtrans_topn_df.where(\"custrno=2\").show()#Second Least transaction made by each customer\n",
    "\n",
    "print(\"Rank & Dense_rank usage:\")\n",
    "custtrans_topn_df=dfcusts.withColumn(\"custrnk\",rank().over(Window.partitionBy(\"custid\").orderBy(asc(\"amt\")))).withColumn(\"custdrnk\",dense_rank().over(Window.partitionBy(\"custid\").orderBy(asc(\"amt\"))))\n",
    "display(custtrans_topn_df)\n",
    "print(\"Rank\")\n",
    "#Rank returns multiple rows per customer if the top transaction is same value\n",
    "custtrans_topn_df.where(\"custrnk=1\").show()#Show me the only top 1 transaction made by the every customer \n",
    "custtrans_topn_df.where(\"custrnk=2\").show()#Second Highest transaction made by each customer\n",
    "custtrans_topn_df.where(\"custrnk=4\").show()#Four Highest transaction made by each customer (row_number will return only 1 row)\n",
    "custtrans_topn_df.where(\"custrnk=5\").show()#this record is missing (because skipped by rank function)\n",
    "custtrans_topn_df.where(\"custrnk=6\").show()#this record is missing (because skipped by rank function)\n",
    "\n",
    "print(\"Dense Rank\")\n",
    "#Dense Rank returns multiple rows per customer if the top transaction is same value and the next value of the rank is dense/not skipped\n",
    "custtrans_topn_df.where(\"custdrnk=1\").show()#Show me the only top 1 transaction made by the every customer \n",
    "custtrans_topn_df.where(\"custdrnk=2\").show()#Second Highest transaction made by each customer\n",
    "custtrans_topn_df.where(\"custdrnk=4\").show()#Fifth Highest transaction made by each customer (row_number will return only 1 row)\n",
    "custtrans_topn_df.where(\"custdrnk=5\").show()#this record is available (because dense rank function is used)\n",
    "custtrans_topn_df.where(\"custdrnk=6\").show()#this record is available (because dense rank function is used)\n",
    "\n",
    "\n",
    "print(\"Rownumber, Rank & Dense_rank usage:\")\n",
    "custtrans_topn_df=dfcusts.withColumn(\"custrno\",row_number().over(Window.partitionBy(\"custid\").orderBy(asc(\"amt\")))).withColumn(\"custrnk\",rank().over(Window.partitionBy(\"custid\").orderBy(asc(\"amt\")))).withColumn(\"custdrnk\",dense_rank().over(Window.partitionBy(\"custid\").orderBy(asc(\"amt\"))))\n",
    "display(custtrans_topn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ac4ae2-2269-4f81-a5d6-27548d08f5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####6.Analytical\n",
    "Functions used for performing some analytical operations on the data in multiple dimensions. We can use windowing or grouping or grouping & aggregation functions to perform analytics/analytical functionalities\n",
    "1. Hierarchical Analytics (lead & lag)\n",
    "2. Aggregation analytics (grouping & aggregations)\n",
    "3. Multi dimensional analytics (rollup, cube, pivot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e139e2-6f72-499c-840e-7882817790f8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767809323133}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Let us perform hierarchical analytics by using lead and lag functions\n",
    "from pyspark.sql.window import Window\n",
    "dfcusts=final_denormalized_df1.select(\"custid\",\"fullname\",\"prof\",\"txnid\",\"amt\",\"prof\",\"txndt\").where(\"custid in (4000001,4000002,4000003)\")\n",
    "#Lead help us arrive the next value\n",
    "hierarchicalleaddf=dfcusts.withColumn(\"nextamt\",lead(\"amt\").over(Window.partitionBy(\"custid\").orderBy(\"txndt\")))\n",
    "display(hierarchicalleaddf)\n",
    "#Lag help us arrive the prior value\n",
    "hierarchicallagdf=dfcusts.withColumn(\"prioramt\",lag(\"amt\").over(Window.partitionBy(\"custid\").orderBy(\"txndt\")))\n",
    "display(hierarchicallagdf)\n",
    "\n",
    "print(\"application of lead and lag\")\n",
    "print(\"How to control the hierarchy (for example I want to compare immediate next transaction or 2 levels of transactions\")\n",
    "#syntax lead(col,offset,default) or lag(col,offset,default)\n",
    "hierarchicalleaddf=dfcusts.withColumn(\"nextamt\",lead(\"amt\",2,-1).over(Window.partitionBy(\"custid\").orderBy(\"txndt\")))\n",
    "display(hierarchicalleaddf)\n",
    "#Lead help us arrive the prior value\n",
    "hierarchicallagdf=dfcusts.withColumn(\"prioramt\",lag(\"amt\",2,0).over(Window.partitionBy(\"custid\").orderBy(\"txndt\")))\n",
    "display(hierarchicallagdf)\n",
    "\n",
    "print(\"Requirement: I want to analyse the purchase pattern/trend of my customer\")\n",
    "hierarchicalleadlagdf=dfcusts.withColumn(\"nextamt\",lead(\"amt\",1,0).over(Window.partitionBy(\"custid\").orderBy(\"txndt\")))\\\n",
    ".withColumn(\"prioramt\",lag(\"amt\",1,0).over(Window.partitionBy(\"custid\").orderBy(\"txndt\")))\n",
    "display(hierarchicalleadlagdf)\n",
    "trendanalysisdf=hierarchicalleadlagdf.withColumn(\"trend\",when((col(\"nextamt\")>col(\"amt\")),\"up\").when((col(\"nextamt\")<col(\"amt\")),\"down\").otherwise(\"same\"))\n",
    "display(trendanalysisdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fbbb9f-ecc0-40f6-8a7d-02e718c6172f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Aggregation analytics (grouping & aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6bbb97f-9cac-4663-bce7-287665ebb7c9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768028199956}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfcusts=final_denormalized_df1.select(\"custid\",\"fullname\",\"txnid\",\"amt\",\"prof\",\"txndt\",\"age\",\"state\",\"city\",\"spendby\").where(\"custid between 4000001 and 4000500\")\n",
    "dfcusts.show(2)\n",
    "groupedaggdf=dfcusts.groupBy(\"prof\").agg(sum(\"amt\").alias(\"sumamt\"),avg(\"age\").alias(\"avgage\"))\n",
    "display(groupedaggdf)\n",
    "groupedaggdf=dfcusts.groupBy(\"state\",\"city\",\"prof\").agg(sum(\"amt\").alias(\"sumamt\"),avg(\"age\").alias(\"avgage\"))\n",
    "display(groupedaggdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcc8fc5-e9b1-44d0-8698-f8c36f69faac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d141d962-7513-4dfe-9351-00631a794c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Multi dimensional analytics (rollup, cube, pivot)\n",
    "print(\"rollup produces the aggregated result in rolled UP dimensions\")\n",
    "groupedaggdf=dfcusts.rollup(\"state\",\"prof\").agg(sum(\"amt\").alias(\"sumamt\"),avg(\"age\").alias(\"avgage\")).orderBy(\"state\",\"prof\")\n",
    "display(groupedaggdf)\n",
    "\n",
    "print(\"cube produces the aggregated result in all dimensions (rollup and rolldown)\")\n",
    "groupedaggdf=dfcusts.cube(\"state\",\"prof\").agg(sum(\"amt\").alias(\"sumamt\"),avg(\"age\").alias(\"avgage\")).orderBy(\"state\",\"prof\")\n",
    "display(groupedaggdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb38dd3e-56f7-45ed-8ff4-e1b1311c4281",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768029119486}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pivot operation (costly function)\n",
    "print(\"pivot produces the aggregated result of the dimensions in a columnar fashion\")\n",
    "groupedaggdf=dfcusts.groupBy(\"state\",\"city\",\"spendby\").agg(sum(\"amt\").alias(\"sumamt\"),avg(\"age\").alias(\"avgage\")).orderBy(\"state\",\"city\",\"spendby\")\n",
    "display(groupedaggdf)\n",
    "groupedaggdf=dfcusts.groupBy(\"state\",\"city\").pivot(\"spendby\").agg(sum(\"amt\").alias(\"sumamt\"),avg(\"age\").alias(\"avgage\")).orderBy(\"state\",\"city\")\n",
    "display(groupedaggdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "078599e3-0497-4850-82e8-c2ae4721147d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####7.Set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5b7ef9-88c1-48a0-977c-64931c395c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Set operation (union, unionAll, insetsect, minus/except/subract)\n",
    "dfcusts1=final_denormalized_df1.select(\"custid\",\"fullname\",\"txnid\",\"amt\",\"prof\",\"txndt\",\"age\",\"state\",\"city\",\"spendby\").where(\"custid between 4000001 and 4000010\")\n",
    "dfcusts2=final_denormalized_df1.select(\"custid\",\"fullname\",\"txnid\",\"amt\",\"prof\",\"txndt\",\"age\",\"state\",\"city\",\"spendby\").where(\"custid between 4000005 and 4000015\")\n",
    "unionalldf=dfcusts1.unionAll(dfcusts2)\n",
    "#display(unionalldf)#returns all data from both the dataframes with duplicates\n",
    "print(unionalldf.count())\n",
    "uniondf=dfcusts1.union(dfcusts2).distinct()\n",
    "display(uniondf)#returns all data from both the dataframes with duplicates  (it will not work like this in spark), we have to use distinct explicitly\n",
    "print(uniondf.count())\n",
    "\n",
    "#intersection\n",
    "commondf=dfcusts1.intersect(dfcusts2)\n",
    "display(commondf)\n",
    "print(commondf.count())\n",
    "\n",
    "#subtract\n",
    "diffdf=dfcusts1.subtract(dfcusts2)#returns df1-df2 data (excessive df1 data)\n",
    "display(diffdf)\n",
    "print(diffdf.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6110b5d-642a-419c-a5e5-38d12864c3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Egress of the required DFs created at any of the above stages, we have persist in the tables or in a form of files or to some external systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bbe034-7274-45e6-ae89-9f83243aeea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. how to create pipelines using different data processing techniques by connecting with different sources/targets\n",
    "2. how to Standardize/Modernization/Industrializing the code and how create/consume generic/reusable functions & frameworks\n",
    "3. Testing (Unit, Peer Review, SIT/Integration, Regression, User Acceptance Testing), Masking engine,\n",
    "4. Reusable transformation(munge_data, optimize_performance),\n",
    "5. Quality suite/Data Profiling/Audit engine (Reconcilation) (Audit framework), Data/process Observability\n",
    "\n",
    "6. how terminologies/architecture/submit jobs/monitor/log analysis/packaging and deployment ...\n",
    "7. performance tuning\n",
    "8. Deploying spark applications in Cloud & other Distributions like Hortonworks/Cloudera/Databricks\n",
    "9. Creating cloud pipelines using spark SQL programs & Cloud native tools\n",
    "\n",
    "What is the importance of learning this program or How this can address interview questions..?\n",
    "VERY VERY IMPORTANT PROGRAM IN TERMS OF EXPLAINING/SOLVING PROBLEMS GIVEN IN INTERVIEW ,\n",
    "WITH THIS ONE PROGRAM YOU CAN COVER ALMOST ALL DATAENGINEERING FEATURES\n",
    "Tell me about the common transformations you performed,\n",
    "tell me your daily roles in DE,\n",
    "tell me some business logics you have writtened recently\n",
    "How do you write an entire spark application,\n",
    "levels/stages of DE pipelines or\n",
    "have you created DE pipelines what are the transformations applied,\n",
    "how many you have created or are you using existing framework or you created some framework?\n",
    "\n",
    "'''\n",
    "TRANSFORMATION & ANALYTICAL TECHNIQUES\n",
    "Starting point - (Data Governance (security) - Tagging, categorization, classification, masking/filteration)\n",
    "1. Data Munging - Process of transforming and mapping data from Raw form into Tidy(usable) format with the\n",
    "intent of making it more appropriate and valuable for a variety of downstream purposes such for\n",
    "further Transformation/Enrichment, Egress/Outbound, analytics, model application & Reporting\n",
    "a. Passive - Data Discovery EDA (Exploratory Data Analytics)\n",
    "(every layers ingestion/transformation/analytics/consumption) -\n",
    "Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns.\n",
    "b. Active - Combining Data + Schema Evolution/Merging (Structuring)\n",
    "c. Validation, Cleansing, Scrubbing - Identifying and filling gaps & Cleaning data to remove outliers and inaccuracies\n",
    "Preprocessing, Preparation\n",
    "Cleansing (removal of unwanted datasets eg. na.drop),\n",
    "Scrubbing (convert of raw to tidy na.fill or na.replace),\n",
    "d. Standardization, De Duplication and Replacement & Deletion of Data to make it in a usable format (Dataengineers/consumers)\n",
    "\n",
    "2. Data Enrichment - Makes your data rich and detailed\n",
    "a. Add, Remove, Rename, Modify/replace\n",
    "b. split, merge/Concat\n",
    "c. Type Casting, format & Schema Migration\n",
    "\n",
    "3. Data Customization & Processing - Application of Tailored Business specific Rules\n",
    "a. User Defined Functions\n",
    "b. Building of Frameworks & Reusable Functions\n",
    "\n",
    "4. Data Curation\n",
    "a. Curation/Transformation\n",
    "b. Analysis/Analytics & Summarization -> filter, transformation, Grouping, Aggregation/Summarization\n",
    "\n",
    "5. Data Wrangling - Gathering, Enriching and Transfomation of pre processed data into usable data\n",
    "a. Lookup/Reference\n",
    "b. Enrichment\n",
    "c. Joins\n",
    "d. Sorting\n",
    "e. Windowing, Statistical & Analytical processing\n",
    "f. Set Operation\n",
    "\n",
    "6. Data Publishing & Consumption - Enablement of the Cleansed, transformed and analysed data as a Data Product.\n",
    "a. Discovery,\n",
    "b. Outbound/Egress,\n",
    "c. Reports/exports\n",
    "d. Schema migration\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d9b849-607e-4852-9687-b41332caa867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_End_End_Transformations_Analytics_DSL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
