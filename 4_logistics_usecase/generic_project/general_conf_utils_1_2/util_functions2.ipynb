{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa1b034d-66c5-472c-a44f-3a002712dd60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This Notebook contains the following functions:<br>\n",
    "1. Python UDF Function\n",
    "2. Generic Framework - Business specific \n",
    "3. Generic Framework - Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675aa3ab-88d2-4556-94ce-a693cc41beba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running Utility Notebook to initialize all functions to use further\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1891f51-6f1b-4f6b-bb4b-b994ba550974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating UDF to convert string to number, hence we don't have to filter string values or manipulate string values manually using dictionary word_to_num={'one':'1','two':'2'}<br>\n",
    "Eg. If we pass \"twenty thousand two hundred and one\" -> 20201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091c523a-36d9-4d3e-afb9-fb90e63708f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install word2number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "906a486e-d5db-4ded-85b9-fe7a51e3d0b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from word2number import w2n\n",
    "\n",
    "def word_to_num(value):\n",
    "    try:\n",
    "        # If already numeric\n",
    "        return int(value)\n",
    "    except:\n",
    "        try:\n",
    "            return w2n.word_to_num(value.lower())\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "word_to_num_udf = udf(word_to_num, IntegerType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d6f659f-ba9e-48be-b4ea-0abfade4809b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Business Specific Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca134f5d-35c9-419f-8d8b-7fa1e19c3509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def standardize_staff(df):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"shipment_id\",word_to_num_udf(F.col(\"shipment_id\")).cast(\"long\"))\n",
    "        .withColumn(\"age\",word_to_num_udf(F.col(\"age\")).cast(\"int\"))\n",
    "        .withColumn(\"role\", F.lower(\"role\"))\n",
    "        .withColumn(\"origin_hub_city\", F.initcap(\"hub_location\"))\n",
    "        .withColumn(\"load_dt\", F.current_timestamp())\n",
    "        .withColumn(\"full_name\", F.concat_ws(\" \", \"first_name\", \"last_name\"))\n",
    "        .withColumn(\"hub_location\", F.initcap(\"hub_location\"))\n",
    "        .drop(\"first_name\", \"last_name\")\n",
    "        .withColumnRenamed(\"full_name\", \"staff_full_name\")\n",
    "    )\n",
    "\n",
    "def scrub_geotag(df):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"city_name\", F.initcap(\"city_name\"))\n",
    "        .withColumn(\"masked_hub_location\", F.initcap(\"country\"))\n",
    "    )\n",
    "\n",
    "def standardize_shipments(df):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"domain\", F.lit(\"Logistics\"))\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"is_expedited\", F.lit(False).cast(\"boolean\"))\n",
    "        .withColumn(\"shipment_date\", F.to_date(\"shipment_date\", \"yy-MM-dd\"))\n",
    "        .withColumn(\"shipment_cost\", F.round(\"shipment_cost\", 2))\n",
    "        .withColumn(\"shipment_weight_kg\", F.col(\"shipment_weight_kg\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "def enrich_shipments(df):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"route_segment\",\n",
    "            F.concat_ws(\"-\", \"source_city\", \"destination_city\"))\n",
    "        .withColumn(\"vehicle_identifier\",\n",
    "            F.concat_ws(\"_\", \"vehicle_type\", \"shipment_id\"))\n",
    "        .withColumn(\"shipment_year\", F.year(\"shipment_date\"))\n",
    "        .withColumn(\"shipment_month\", F.month(\"shipment_date\"))\n",
    "        .withColumn(\"is_weekend\",\n",
    "            F.dayofweek(\"shipment_date\").isin([1,7]))\n",
    "        .withColumn(\"is_expedited\",\n",
    "            F.col(\"shipment_status\").isin(\"IN_TRANSIT\", \"DELIVERED\"))\n",
    "        .withColumn(\"cost_per_kg\",\n",
    "            F.round(F.col(\"shipment_cost\") / F.col(\"shipment_weight_kg\"), 2))\n",
    "        .withColumn(\"tax_amount\",\n",
    "            F.round(F.col(\"shipment_cost\") * 0.18, 2))\n",
    "        .withColumn(\"days_since_shipment\",\n",
    "            F.datediff(F.current_date(), \"shipment_date\"))\n",
    "        .withColumn(\"is_high_value\",\n",
    "            F.col(\"shipment_cost\") > 50000)\n",
    "    )\n",
    "\n",
    "def split_columns(df):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"order_prefix\", F.substring(\"order_id\", 1, 3))\n",
    "        .withColumn(\"order_sequence\", F.substring(\"order_id\", 4, 10))\n",
    "        .withColumn(\"ship_year\", F.year(\"shipment_date\"))\n",
    "        .withColumn(\"ship_month\", F.month(\"shipment_date\"))\n",
    "        .withColumn(\"ship_day\", F.dayofmonth(\"shipment_date\"))\n",
    "        .withColumn(\"route_lane\",\n",
    "            F.concat_ws(\"->\", \"source_city\", \"destination_city\"))\n",
    "    )\n",
    "\n",
    "def mask_name(col):\n",
    "    return F.concat(\n",
    "        F.substring(col, 1, 2),\n",
    "        F.lit(\"****\"),\n",
    "        F.substring(col, -1, 1)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa396482-29cd-4a3d-a9ce-c3b06d081453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c42bf4a-cd7e-4685-aca7-a5d7b171f822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Return Spark session\n",
    "from pyspark.sql.session import SparkSession\n",
    "def get_spark_session(app_name=\"Some Anonymous Data Engineering Project\"):\n",
    "    try:\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            return spark\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (SparkSession.builder.config(\"spark.sql.shuffle.partitions\", \"1\").appName(app_name).getOrCreate())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342ddd87-0e33-4fb8-af94-49a5efaaddc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#All generic functions for reading data from files & tables\n",
    "\n",
    "def read_csv_df(spark,path,header=True,infer_schema=True,sep=\",\"):\n",
    "    return_df=spark.read.option(\"header\", header).option(\"inferSchema\", infer_schema)\\\n",
    "        .option(\"sep\", sep)\\\n",
    "        .csv(path)\n",
    "    return return_df\n",
    "\n",
    "def read_json_df(spark, path,mline=True):\n",
    "    return spark.read.json(path,multiLine=mline,mode=\"PERMISSIVE\")\n",
    "\n",
    "   \n",
    "def read_delta_df(spark,path):\n",
    "    return spark.read.format(\"delta\").load(path)\n",
    "\n",
    "def read_file(spark,filetype,path,header=True,infer_schema=True,mline=True):\n",
    "    if filetype==\"csv\":\n",
    "        return spark.read.csv(path,header=header,inferSchema=infer_schema)#read_csv_df(spark,path)\n",
    "    elif filetype==\"json\":\n",
    "        return read_json_df(spark,path)\n",
    "    elif filetype==\"delta\":\n",
    "        return read_delta_df(spark,path)\n",
    "    elif filetype=='orc':\n",
    "        return spark.read.orc(path)\n",
    "    else:\n",
    "        raise Exception(\"File type not supported\")\n",
    "\n",
    "\n",
    "def read_table(spark,table_name):\n",
    "    return spark.table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c15ae68-11f5-4ff6-a474-ea9358fa5dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Return Joined DF\n",
    "def join_df(df1,df2,how=\"inner\",on=\"shipment_id\"):#To avoid cartesian/cross join, i am adding some column in the on\n",
    "    return df1.join(df2, on=on, how=how)\n",
    "\n",
    "def unionDf(df1,df2):\n",
    "    return df1.union(df2)\n",
    "def unionDfSql(spark,view1,view2):    \n",
    "    returndf=spark.sql(f\"select * from view1 union select * from view2\")\n",
    "    return returndf\n",
    "\n",
    "def mergeDf(df1,df2,allowmissingcol=True):\n",
    "    return df1.unionByName(df2, allowMissingColumns=allowmissingcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ec6b70-4d7e-4ffc-a105-3a044815366f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "def add_literal_columns(df, columns, default_value=None):\n",
    "    for col_name in columns:\n",
    "        df = df.withColumn(col_name, lit(default_value))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8571e16-3f59-4f75-80d9-7c1076b4a14c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#All generic functions for writing data to files(datalake) & tables(lakehouse)\n",
    "\n",
    "def write_file(df, path, mode=\"overwrite\", format=\"delta\"):\n",
    "    return df.write.mode(mode).format(format).save(path)\n",
    "\n",
    "def write_table(df, tablename, mode=\"overwrite\"):\n",
    "    df.write.mode(mode).format(\"delta\").saveAsTable(tablename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc72ca20-b935-40ff-829e-df8d5ef4ed26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def return_tempview_distinct_df(spark,tempview):\n",
    "    return spark.sql(f\"select distinct * from {tempview}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "util_functions2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
